{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAYMF1OL1tzO"
      },
      "source": [
        "## Get the TensorRT tar file before running this Notebook\n",
        "\n",
        "1. Visit https://developer.nvidia.com/tensorrt\n",
        "2. Clicking `Download now` from step one directs you to https://developer.nvidia.com/nvidia-tensorrt-download where you have to Login/Join Now for Nvidia Developer Program Membership\n",
        "3. Now, in the download page: Choose TensorRT 8 in available versions\n",
        "4. Agree to Terms and Conditions\n",
        "5. Click on TensorRT 8.6 GA to expand the available options\n",
        "6. Click on 'TensorRT 8.6 GA for Linux x86_64 and CUDA 12.0 and 12.1 TAR Package' to dowload the TAR file\n",
        "7. Upload the the tar file to your Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGLBrzF8hKgS"
      },
      "source": [
        "## Connect to GPU Instance\n",
        "\n",
        "1. Change Runtime type to GPU by Runtime(Top Left tab)->Change Runtime Type->GPU(Hardware Accelerator)\n",
        "1. Then click on Connect (Top Right)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjpjyNg5c2V9"
      },
      "source": [
        "## Mounting Google drive\n",
        "Mount your Google drive storage to this Colab instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvUVkYw0hzqG",
        "outputId": "a78629fc-8e61-4836-9fbe-806ce88b4724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: GOOGLE_COLAB=1\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    %env GOOGLE_COLAB=1\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "else:\n",
        "    %env GOOGLE_COLAB=0\n",
        "    print(\"Warning: Not a Colab Environment\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "5ZzyiDQ09mIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IZtVf3cPyom"
      },
      "source": [
        "# License Plate Recognition using TAO LPRNet\n",
        "\n",
        "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task.\n",
        "\n",
        "Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
        "\n",
        "<img align=\"center\" src=\"https://developer.nvidia.com/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png\" width=\"1080\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxZQEfTBPyoq"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n",
        "\n",
        "* Take a pretrained baseline18 LPRNet model and train it on the OpenALPR benchmark dataset\n",
        "* Run Inference on the trained model\n",
        "* Export the trained model to a .etlt file for deployment to DeepStream\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "This notebook shows an example usecase of LPRNet using Train Adapt Optimize (TAO) Toolkit.\n",
        "\n",
        "0. [Set up env variables](#head-0)\n",
        "1. [Prepare dataset and pre-trained model](#head-1) <br>\n",
        "    1.1 [Download pre-trained model](#head-1-1) <br>\n",
        "2. [Setup GPU environment](#head-2) <br>\n",
        "    2.1 [Setup Python environment](#head-2-1) <br>\n",
        "3. [Provide training specification](#head-3)\n",
        "4. [Run TAO training](#head-4)\n",
        "5. [Evaluate trained models](#head-5)\n",
        "6. [Inferences](#head-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5lxlv5IPyoq"
      },
      "source": [
        "#### Note\n",
        "1. This notebook currently is by default set up to run training using 1 GPU. To use more GPU's please update the env variable `$NUM_GPUS` accordingly\n",
        "2. This notebook uses OPENALPR dataset by default, which should be around ~2.2 MB.\n",
        "3. Using the default config/spec file provided in this notebook, each weight file size of lprnet created during training will be ~111 MB\n",
        "\n",
        "## 0. Set up env variables and set FIXME parameters <a class=\"anchor\" id=\"head-0\"></a>\n",
        "\n",
        "*Note: This notebook currently is by default set up to run training using 1 GPU. To use more GPU's please update the env variable `$NUM_GPUS` accordingly*\n",
        "\n",
        "#### FIXME\n",
        "1. NUM_GPUS - set this to <= number of GPU's availble on the instance\n",
        "1. GPU_INDEX - set to to the indices of the GPU available on the instance\n",
        "1. COLAB_NOTEBOOKS_PATH - for Google Colab environment, set this path where you want to clone the repo to; for local system environment, set this path to the already cloned repo\n",
        "1. EXPERIMENT_DIR - set this path to a folder location where pretrained models, checkpoints and log files during different model actions will be saved\n",
        "1. delete_existing_experiments - set to True to remove existing pretrained models, checkpoints and log files of a previous experiment\n",
        "1. DATA_DIR - set this path to a folder location where you want to dataset to be present\n",
        "1. delete_existing_data - set this to True to remove existing preprocessed and original data\n",
        "1. trt_tar_path - set this path of the uploaded TensorRT tar.gz file after browser download\n",
        "1. trt_untar_folder_path - set to path of the folder where the TensoRT tar.gz file has to be untarred into\n",
        "1. trt_version - set this to the version of TRT you have downloaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dXT1Uv__Pyor",
        "outputId": "edb38e0e-39fd-44a0-d784-0c039a2ce230",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TAO_DOCKER_DISABLE=1\n",
            "env: KEY=nvidia_tlt\n",
            "env: NUM_GPUS=1\n",
            "env: GPU_INDEX=0\n",
            "env: COLAB_NOTEBOOKS_PATH=/content/drive/MyDrive/nvidia-tao\n",
            "env: EXPERIMENT_DIR=/content/drive/MyDrive/results/lprnet\n",
            "env: DATA_DIR=/content/drive/MyDrive/lprnet_data/\n",
            "env: SPECS_DIR=/content/drive/MyDrive/nvidia-tao/tensorflow/lprnet/specs\n",
            "total 2\n",
            "-rw------- 1 root root   70 Oct 30 09:49 us_lp_characters.txt\n",
            "-rw------- 1 root root 1137 Oct 30 09:49 tutorial_spec.txt\n"
          ]
        }
      ],
      "source": [
        "# Setting up env variables for cleaner command line commands.\n",
        "import os\n",
        "\n",
        "%env TAO_DOCKER_DISABLE=1\n",
        "\n",
        "%env KEY=nvidia_tlt\n",
        "#FIXME1\n",
        "%env NUM_GPUS=1\n",
        "#FIXME2\n",
        "%env GPU_INDEX=0\n",
        "\n",
        "#FIXME3\n",
        "%env COLAB_NOTEBOOKS_PATH=/content/drive/MyDrive/nvidia-tao\n",
        "if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n",
        "    if not os.path.exists(os.path.join(os.environ[\"COLAB_NOTEBOOKS_PATH\"])):\n",
        "\n",
        "      !git clone https://github.com/NVIDIA-AI-IOT/nvidia-tao.git $COLAB_NOTEBOOKS_PATH\n",
        "else:\n",
        "    if not os.path.exists(os.environ[\"COLAB_NOTEBOOKS_PATH\"]):\n",
        "        raise Exception(\"Error, enter the path of the colab notebooks repo correctly\")\n",
        "\n",
        "#FIXME4\n",
        "%env EXPERIMENT_DIR=/content/drive/MyDrive/results/lprnet\n",
        "#FIXME5\n",
        "delete_existing_experiments = True\n",
        "#FIXME6\n",
        "%env DATA_DIR=/content/drive/MyDrive/lprnet_data/\n",
        "#FIXME7\n",
        "delete_existing_data = False\n",
        "\n",
        "if delete_existing_experiments:\n",
        "    !sudo rm -rf $EXPERIMENT_DIR\n",
        "if delete_existing_data:\n",
        "    !sudo rm -rf $DATA_DIR\n",
        "\n",
        "SPECS_DIR=f\"{os.environ['COLAB_NOTEBOOKS_PATH']}/tensorflow/lprnet/specs\"\n",
        "%env SPECS_DIR={SPECS_DIR}\n",
        "# Showing list of specification files.\n",
        "!ls -rlt $SPECS_DIR\n",
        "\n",
        "!sudo mkdir -p $DATA_DIR && sudo chmod -R 777 $DATA_DIR\n",
        "!sudo mkdir -p $EXPERIMENT_DIR && sudo chmod -R 777 $EXPERIMENT_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI_8N9_IPyov"
      },
      "source": [
        "## 1. Prepare dataset and pre-trained model <a class=\"anchor\" id=\"head-1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvQbbU5wPyov"
      },
      "source": [
        " We will be using the OpenALPR benchmark dataset for the tutorial. The following script will download the dataset automatically and convert it to the format used by TAO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kBI24bHSPyov",
        "outputId": "aafa58f2-6063-400f-f040-d65ac7312577",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+ '[' -z /content/drive/MyDrive/lprnet_data/ ']'\n",
            "++ pwd\n",
            "+ CURRENT_DIR=/content\n",
            "+ echo 'Cloning OpenALPR benchmark directory'\n",
            "Cloning OpenALPR benchmark directory\n",
            "+ '[' '!' -e benchmarks ']'\n",
            "+ git clone https://github.com/openalpr/benchmarks benchmarks\n",
            "Cloning into 'benchmarks'...\n",
            "remote: Enumerating objects: 1752, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 1752 (delta 22), reused 22 (delta 22), pack-reused 1728 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1752/1752), 187.98 MiB | 26.87 MiB/s, done.\n",
            "Resolving deltas: 100% (35/35), done.\n",
            "Updating files: 100% (1707/1707), done.\n",
            "+ OUTPUT_DIR=/content/drive/MyDrive/lprnet_data\n",
            "+ mkdir -p /content/drive/MyDrive/lprnet_data\n",
            "+++ readlink -f /content/drive/MyDrive/nvidia-tao/tensorflow/lprnet/download_and_prepare_data.sh\n",
            "++ dirname /content/drive/MyDrive/nvidia-tao/tensorflow/lprnet/download_and_prepare_data.sh\n",
            "+ SCRIPT_DIR=/content/drive/MyDrive/nvidia-tao/tensorflow/lprnet\n",
            "+ echo 'Preprocessing OpenALPR benchmarks data for US'\n",
            "Preprocessing OpenALPR benchmarks data for US\n",
            "+ python3 /content/drive/MyDrive/nvidia-tao/tensorflow/lprnet/preprocess_openalpr_benchmark.py --input_dir=/content/benchmarks/endtoend/us/ --output_dir=/content/drive/MyDrive/lprnet_data\n",
            "Total 222 samples in benchmark dataset\n",
            "111 for train and 111 for val\n"
          ]
        }
      ],
      "source": [
        "!bash $COLAB_NOTEBOOKS_PATH/tensorflow/lprnet/download_and_prepare_data.sh $DATA_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Oi9ACFA0Pyov",
        "outputId": "1b3e82f7-46d5-466e-8638-4eefac4eccf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/lprnet_data/\n",
            "total 8\n",
            "drwx------ 4 root root 4096 Oct 30 09:56 train\n",
            "drwx------ 4 root root 4096 Oct 30 09:56 val\n",
            "total 8\n",
            "drwx------ 2 root root 4096 Oct 30 09:56 image\n",
            "drwx------ 2 root root 4096 Oct 30 09:56 label\n",
            "total 511\n",
            "-rw------- 1 root root  5009 Oct 30 09:56 0b86cecf-67d1-4fc0-87c9-b36b0ee228bb.jpg\n",
            "-rw------- 1 root root  1667 Oct 30 09:56 12c6cb72-3ea3-49e7-b381-e0cdfc5e8960.jpg\n",
            "-rw------- 1 root root  1882 Oct 30 09:56 1e241dc8-8f18-4955-8988-03a0ab49f813.jpg\n",
            "-rw------- 1 root root  2092 Oct 30 09:56 22e54a62-57a8-4a0a-88c1-4b9758f67651.jpg\n",
            "-rw------- 1 root root  2003 Oct 30 09:56 33fa5185-0286-4e8f-b775-46162eba39d4.jpg\n",
            "-rw------- 1 root root  2092 Oct 30 09:56 37170dd1-2802-4e38-b982-c5d07c64ff67.jpg\n",
            "-rw------- 1 root root  2067 Oct 30 09:56 4be2025c-09f7-4bb0-b1bd-8e8633e6dec1.jpg\n",
            "-rw------- 1 root root  1860 Oct 30 09:56 5b562a61-34ad-4f00-9164-d34abb7a38e4.jpg\n",
            "-rw------- 1 root root  3431 Oct 30 09:56 car14.jpg\n",
            "-rw------- 1 root root  5296 Oct 30 09:56 car1.jpg\n",
            "-rw------- 1 root root  1700 Oct 30 09:56 car21.jpg\n",
            "-rw------- 1 root root  1650 Oct 30 09:56 car22.jpg\n",
            "-rw------- 1 root root  5366 Oct 30 09:56 car3.jpg\n",
            "-rw------- 1 root root  3306 Oct 30 09:56 car5.jpg\n",
            "-rw------- 1 root root  2748 Oct 30 09:56 car6.jpg\n",
            "-rw------- 1 root root  2537 Oct 30 09:56 car8.jpg\n",
            "-rw------- 1 root root 11997 Oct 30 09:56 car9-1.jpg\n",
            "-rw------- 1 root root  3095 Oct 30 09:56 car9-2.jpg\n",
            "-rw------- 1 root root  4836 Oct 30 09:56 car9-4.jpg\n",
            "-rw------- 1 root root  3125 Oct 30 09:56 car9.jpg\n",
            "-rw------- 1 root root  1974 Oct 30 09:56 cfaa9dd2-a388-4e92-bb3a-ae65c28d8139.jpg\n",
            "-rw------- 1 root root  2112 Oct 30 09:56 d4f79480-366a-40b6-ab2c-328bcba705b2.jpg\n",
            "-rw------- 1 root root  4663 Oct 30 09:56 us2.jpg\n",
            "-rw------- 1 root root 13834 Oct 30 09:56 us5.jpg\n",
            "-rw------- 1 root root 14499 Oct 30 09:56 us6.jpg\n",
            "-rw------- 1 root root 21030 Oct 30 09:56 us7.jpg\n",
            "-rw------- 1 root root 38963 Oct 30 09:56 us8.jpg\n",
            "-rw------- 1 root root  4466 Oct 30 09:56 wts-lg-000011.jpg\n",
            "-rw------- 1 root root  2169 Oct 30 09:56 wts-lg-000014.jpg\n",
            "-rw------- 1 root root  4342 Oct 30 09:56 wts-lg-000024.jpg\n",
            "-rw------- 1 root root  3010 Oct 30 09:56 wts-lg-000025.jpg\n",
            "-rw------- 1 root root  4065 Oct 30 09:56 wts-lg-000026.jpg\n",
            "-rw------- 1 root root  6964 Oct 30 09:56 wts-lg-000027.jpg\n",
            "-rw------- 1 root root  8334 Oct 30 09:56 wts-lg-000028.jpg\n",
            "-rw------- 1 root root  4293 Oct 30 09:56 wts-lg-000031.jpg\n",
            "-rw------- 1 root root  1882 Oct 30 09:56 wts-lg-000036.jpg\n",
            "-rw------- 1 root root  2849 Oct 30 09:56 wts-lg-000038.jpg\n",
            "-rw------- 1 root root  3907 Oct 30 09:56 wts-lg-000041.jpg\n",
            "-rw------- 1 root root  3343 Oct 30 09:56 wts-lg-000043.jpg\n",
            "-rw------- 1 root root  2448 Oct 30 09:56 wts-lg-000045.jpg\n",
            "-rw------- 1 root root  2748 Oct 30 09:56 wts-lg-000048.jpg\n",
            "-rw------- 1 root root  2309 Oct 30 09:56 wts-lg-000050.jpg\n",
            "-rw------- 1 root root  2773 Oct 30 09:56 wts-lg-000052.jpg\n",
            "-rw------- 1 root root  2040 Oct 30 09:56 wts-lg-000053.jpg\n",
            "-rw------- 1 root root  1929 Oct 30 09:56 wts-lg-000055.jpg\n",
            "-rw------- 1 root root  3038 Oct 30 09:56 wts-lg-000056.jpg\n",
            "-rw------- 1 root root  3636 Oct 30 09:56 wts-lg-000057.jpg\n",
            "-rw------- 1 root root  3535 Oct 30 09:56 wts-lg-000060.jpg\n",
            "-rw------- 1 root root  2670 Oct 30 09:56 wts-lg-000061.jpg\n",
            "-rw------- 1 root root  2590 Oct 30 09:56 wts-lg-000063.jpg\n",
            "-rw------- 1 root root  4655 Oct 30 09:56 wts-lg-000065.jpg\n",
            "-rw------- 1 root root  3471 Oct 30 09:56 wts-lg-000066.jpg\n",
            "-rw------- 1 root root  2069 Oct 30 09:56 wts-lg-000067.jpg\n",
            "-rw------- 1 root root  4094 Oct 30 09:56 wts-lg-000070.jpg\n",
            "-rw------- 1 root root  3319 Oct 30 09:56 wts-lg-000073.jpg\n",
            "-rw------- 1 root root  4047 Oct 30 09:56 wts-lg-000074.jpg\n",
            "-rw------- 1 root root  3922 Oct 30 09:56 wts-lg-000076.jpg\n",
            "-rw------- 1 root root  3268 Oct 30 09:56 wts-lg-000078.jpg\n",
            "-rw------- 1 root root  5053 Oct 30 09:56 wts-lg-000080.jpg\n",
            "-rw------- 1 root root  3043 Oct 30 09:56 wts-lg-000085.jpg\n",
            "-rw------- 1 root root  1887 Oct 30 09:56 wts-lg-000088.jpg\n",
            "-rw------- 1 root root  3370 Oct 30 09:56 wts-lg-000090.jpg\n",
            "-rw------- 1 root root  2293 Oct 30 09:56 wts-lg-000091.jpg\n",
            "-rw------- 1 root root  2450 Oct 30 09:56 wts-lg-000095.jpg\n",
            "-rw------- 1 root root  3622 Oct 30 09:56 wts-lg-000097.jpg\n",
            "-rw------- 1 root root  3838 Oct 30 09:56 wts-lg-000098.jpg\n",
            "-rw------- 1 root root  3725 Oct 30 09:56 wts-lg-000099.jpg\n",
            "-rw------- 1 root root  3084 Oct 30 09:56 wts-lg-000101.jpg\n",
            "-rw------- 1 root root  2654 Oct 30 09:56 wts-lg-000102.jpg\n",
            "-rw------- 1 root root  3483 Oct 30 09:56 wts-lg-000115.jpg\n",
            "-rw------- 1 root root  3464 Oct 30 09:56 wts-lg-000119.jpg\n",
            "-rw------- 1 root root  3442 Oct 30 09:56 wts-lg-000120.jpg\n",
            "-rw------- 1 root root  2286 Oct 30 09:56 wts-lg-000121.jpg\n",
            "-rw------- 1 root root  2512 Oct 30 09:56 wts-lg-000124.jpg\n",
            "-rw------- 1 root root  2357 Oct 30 09:56 wts-lg-000126.jpg\n",
            "-rw------- 1 root root  3239 Oct 30 09:56 wts-lg-000127.jpg\n",
            "-rw------- 1 root root  6309 Oct 30 09:56 wts-lg-000129.jpg\n",
            "-rw------- 1 root root  3119 Oct 30 09:56 wts-lg-000131.jpg\n",
            "-rw------- 1 root root  3997 Oct 30 09:56 wts-lg-000134.jpg\n",
            "-rw------- 1 root root  6167 Oct 30 09:56 wts-lg-000135.jpg\n",
            "-rw------- 1 root root  7349 Oct 30 09:56 wts-lg-000137.jpg\n",
            "-rw------- 1 root root  2880 Oct 30 09:56 wts-lg-000141.jpg\n",
            "-rw------- 1 root root  4233 Oct 30 09:56 wts-lg-000142.jpg\n",
            "-rw------- 1 root root  7493 Oct 30 09:56 wts-lg-000144.jpg\n",
            "-rw------- 1 root root  3232 Oct 30 09:56 wts-lg-000145.jpg\n",
            "-rw------- 1 root root  4131 Oct 30 09:56 wts-lg-000148.jpg\n",
            "-rw------- 1 root root  3751 Oct 30 09:56 wts-lg-000149.jpg\n",
            "-rw------- 1 root root  3020 Oct 30 09:56 wts-lg-000151.jpg\n",
            "-rw------- 1 root root  7538 Oct 30 09:56 wts-lg-000158.jpg\n",
            "-rw------- 1 root root  3873 Oct 30 09:56 wts-lg-000160.jpg\n",
            "-rw------- 1 root root  4598 Oct 30 09:56 wts-lg-000161.jpg\n",
            "-rw------- 1 root root  4186 Oct 30 09:56 wts-lg-000163.jpg\n",
            "-rw------- 1 root root  7254 Oct 30 09:56 wts-lg-000164.jpg\n",
            "-rw------- 1 root root  7106 Oct 30 09:56 wts-lg-000165.jpg\n",
            "-rw------- 1 root root  4216 Oct 30 09:56 wts-lg-000167.jpg\n",
            "-rw------- 1 root root  8129 Oct 30 09:56 wts-lg-000168.jpg\n",
            "-rw------- 1 root root  4325 Oct 30 09:56 wts-lg-000170.jpg\n",
            "-rw------- 1 root root  3706 Oct 30 09:56 wts-lg-000173.jpg\n",
            "-rw------- 1 root root  4448 Oct 30 09:56 wts-lg-000174.jpg\n",
            "-rw------- 1 root root  7503 Oct 30 09:56 wts-lg-000175.jpg\n",
            "-rw------- 1 root root  8097 Oct 30 09:56 wts-lg-000176.jpg\n",
            "-rw------- 1 root root  3419 Oct 30 09:56 wts-lg-000178.jpg\n",
            "-rw------- 1 root root  1933 Oct 30 09:56 wts-lg-000183.jpg\n",
            "-rw------- 1 root root  2286 Oct 30 09:56 wts-lg-000187.jpg\n",
            "-rw------- 1 root root  2895 Oct 30 09:56 wts-lg-000188.jpg\n",
            "-rw------- 1 root root  2049 Oct 30 09:56 wts-lg-000189.jpg\n",
            "-rw------- 1 root root  6398 Oct 30 09:56 wts-lg-000193.jpg\n",
            "-rw------- 1 root root  3890 Oct 30 09:56 wts-lg-000195.jpg\n",
            "-rw------- 1 root root  6452 Oct 30 09:56 wts-lg-000196.jpg\n",
            "-rw------- 1 root root  2671 Oct 30 09:56 wts-lg-000197.jpg\n",
            "-rw------- 1 root root  2762 Oct 30 09:56 wts-lg-000199.jpg\n",
            "total 56\n",
            "-rw------- 1 root root 6 Oct 30 09:56 0b86cecf-67d1-4fc0-87c9-b36b0ee228bb.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 12c6cb72-3ea3-49e7-b381-e0cdfc5e8960.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 1e241dc8-8f18-4955-8988-03a0ab49f813.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 22e54a62-57a8-4a0a-88c1-4b9758f67651.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 33fa5185-0286-4e8f-b775-46162eba39d4.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 37170dd1-2802-4e38-b982-c5d07c64ff67.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 4be2025c-09f7-4bb0-b1bd-8e8633e6dec1.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 5b562a61-34ad-4f00-9164-d34abb7a38e4.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 car14.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 car1.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 car21.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 car22.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 car3.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 car5.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 car6.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 car8.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 car9-1.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 car9-2.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 car9-4.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 car9.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 cfaa9dd2-a388-4e92-bb3a-ae65c28d8139.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 d4f79480-366a-40b6-ab2c-328bcba705b2.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 us2.txt\n",
            "-rw------- 1 root root 8 Oct 30 09:56 us5.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 us6.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 us7.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 us8.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000011.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000014.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000024.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000025.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000026.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000027.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000028.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000031.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000036.txt\n",
            "-rw------- 1 root root 5 Oct 30 09:56 wts-lg-000038.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000041.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000043.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000045.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000048.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000050.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000052.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000053.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000055.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000056.txt\n",
            "-rw------- 1 root root 5 Oct 30 09:56 wts-lg-000057.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000060.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000061.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000063.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000065.txt\n",
            "-rw------- 1 root root 5 Oct 30 09:56 wts-lg-000066.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000067.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000070.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000073.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000074.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000076.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000078.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000080.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000085.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000088.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000090.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000091.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000095.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000097.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000098.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000099.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000101.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000102.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000115.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000119.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000120.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000121.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000124.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000126.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000127.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000129.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000131.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000134.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000135.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000137.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000141.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000142.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000144.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000145.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000148.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000149.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000151.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000158.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000160.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000161.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000163.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000164.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000165.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000167.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000168.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000170.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000173.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000174.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000175.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000176.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000178.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000183.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000187.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000188.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000189.txt\n",
            "-rw------- 1 root root 6 Oct 30 09:56 wts-lg-000193.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000195.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000196.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000197.txt\n",
            "-rw------- 1 root root 7 Oct 30 09:56 wts-lg-000199.txt\n"
          ]
        }
      ],
      "source": [
        "# verify\n",
        "!echo $DATA_DIR\n",
        "!ls -l $DATA_DIR/\n",
        "!ls -l $DATA_DIR/train\n",
        "!ls -l $DATA_DIR/train/image\n",
        "!ls -l $DATA_DIR/train/label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2B28ZwpPyox"
      },
      "source": [
        "### 1.1 Download pretrained model from NGC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU1c75iPPyox"
      },
      "source": [
        "We will use NGC CLI to get the pre-trained models. For more details, go to https://ngc.nvidia.com and click the SETUP on the navigation bar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "85pHGpdOPyoy",
        "outputId": "b77b19f1-b1dd-4737-b077-0bd0f2c79148",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: LOCAL_PROJECT_DIR=/ngc_content/\n",
            "env: CLI=ngccli_cat_linux.zip\n",
            "--2024-10-30 09:57:08--  https://api.ngc.nvidia.com/v2/resources/nvidia/ngc-apps/ngc_cli/versions/3.23.0/files/ngccli_linux.zip\n",
            "Resolving api.ngc.nvidia.com (api.ngc.nvidia.com)... 50.112.83.35, 35.166.74.86\n",
            "Connecting to api.ngc.nvidia.com (api.ngc.nvidia.com)|50.112.83.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://xfiles.ngc.nvidia.com/org/nvidia/team/ngc-apps/recipes/ngc_cli/versions/3.23.0/files/ngccli_linux.zip?versionId=Bpzrduq29jxiO6V_pwHtxB_RuGz7cqzb&Expires=1730368628&Signature=cPaL9lw6xf8zM4kXedwL5CNXF~s0geQ3I1yFsLMlFiFzwv68DnoI5bY2DxbmyJPVDPATQO6ll-XQtfWi1pocwxklKyGjcdD6CIAm~85wNAlTCjdrZA0cWaA7WH6ACOH443umUEVwwiRy6eLWk9Idwidhx82~dRZrETfrwQGQbUanAqms35mNkyRAe~D2QiQSAOUJcukCgts7ElK8UvaOGZ~0sysnZjxNynd9X5K6rWJSCXcyeAMBIUdVNYhzBwKJT2KlyZ2SiCSZb9FMJnp0AEeuvluNDqcUNPcTXOzjCGCkyoJm3FTEiGRDzi2SDB~hAfR7DnrOp3Txc4W5rdG8UA__&Key-Pair-Id=KCX06E8E9L60W [following]\n",
            "--2024-10-30 09:57:08--  https://xfiles.ngc.nvidia.com/org/nvidia/team/ngc-apps/recipes/ngc_cli/versions/3.23.0/files/ngccli_linux.zip?versionId=Bpzrduq29jxiO6V_pwHtxB_RuGz7cqzb&Expires=1730368628&Signature=cPaL9lw6xf8zM4kXedwL5CNXF~s0geQ3I1yFsLMlFiFzwv68DnoI5bY2DxbmyJPVDPATQO6ll-XQtfWi1pocwxklKyGjcdD6CIAm~85wNAlTCjdrZA0cWaA7WH6ACOH443umUEVwwiRy6eLWk9Idwidhx82~dRZrETfrwQGQbUanAqms35mNkyRAe~D2QiQSAOUJcukCgts7ElK8UvaOGZ~0sysnZjxNynd9X5K6rWJSCXcyeAMBIUdVNYhzBwKJT2KlyZ2SiCSZb9FMJnp0AEeuvluNDqcUNPcTXOzjCGCkyoJm3FTEiGRDzi2SDB~hAfR7DnrOp3Txc4W5rdG8UA__&Key-Pair-Id=KCX06E8E9L60W\n",
            "Resolving xfiles.ngc.nvidia.com (xfiles.ngc.nvidia.com)... 3.171.22.62, 3.171.22.96, 3.171.22.57, ...\n",
            "Connecting to xfiles.ngc.nvidia.com (xfiles.ngc.nvidia.com)|3.171.22.62|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 43271011 (41M) [binary/octet-stream]\n",
            "Saving to: ‘/ngc_content//ngccli/ngccli_cat_linux.zip’\n",
            "\n",
            "ngccli_cat_linux.zi 100%[===================>]  41.27M   166MB/s    in 0.2s    \n",
            "\n",
            "2024-10-30 09:57:09 (166 MB/s) - ‘/ngc_content//ngccli/ngccli_cat_linux.zip’ saved [43271011/43271011]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Installing NGC CLI on the local machine.\n",
        "## Download and install\n",
        "%env LOCAL_PROJECT_DIR=/ngc_content/\n",
        "%env CLI=ngccli_cat_linux.zip\n",
        "!sudo mkdir -p $LOCAL_PROJECT_DIR/ngccli && sudo chmod -R 777 $LOCAL_PROJECT_DIR\n",
        "\n",
        "# Remove any previously existing CLI installations\n",
        "!sudo rm -rf $LOCAL_PROJECT_DIR/ngccli/*\n",
        "!wget --content-disposition 'https://api.ngc.nvidia.com/v2/resources/nvidia/ngc-apps/ngc_cli/versions/3.23.0/files/ngccli_linux.zip' -P $LOCAL_PROJECT_DIR/ngccli -O $LOCAL_PROJECT_DIR/ngccli/$CLI\n",
        "!unzip -u -q \"$LOCAL_PROJECT_DIR/ngccli/$CLI\" -d $LOCAL_PROJECT_DIR/ngccli/\n",
        "!rm $LOCAL_PROJECT_DIR/ngccli/*.zip\n",
        "os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"LOCAL_PROJECT_DIR\", \"\"), os.getenv(\"PATH\", \"\"))\n",
        "!cp /usr/lib/x86_64-linux-gnu/libstdc++.so.6 $LOCAL_PROJECT_DIR/ngccli/ngc-cli/libstdc++.so.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IcsQ_FWqPyoy",
        "outputId": "091f09fb-9835-4438-fed5-f45cb276797e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLI_VERSION: Latest - 3.53.0 available (current: 3.23.0). Please update by using the command 'ngc version upgrade' \n",
            "\n",
            "+----------+----------+--------+-------+-------+----------+--------+----------+---------+\n",
            "| Version  | Accuracy | Epochs | Batch | GPU   | Memory F | File   | Status   | Created |\n",
            "|          |          |        | Size  | Model | ootprint | Size   |          | Date    |\n",
            "+----------+----------+--------+-------+-------+----------+--------+----------+---------+\n",
            "| trainabl | 99.67    | 120    | 1     | V100  | 221.1    | 221.06 | UPLOAD_C | Aug 24, |\n",
            "| e_v1.0   |          |        |       |       |          | MB     | OMPLETE  | 2021    |\n",
            "| deployab | 99.67    | 120    | 1     | V100  | 110.1    | 110.09 | UPLOAD_C | Aug 24, |\n",
            "| le_v1.0  |          |        |       |       |          | MB     | OMPLETE  | 2021    |\n",
            "| deployab |          |        |       |       |          | 110.09 | UPLOAD_C | Sep 20, |\n",
            "| le_onnx_ |          |        |       |       |          | MB     | OMPLETE  | 2024    |\n",
            "| v1.1     |          |        |       |       |          |        |          |         |\n",
            "+----------+----------+--------+-------+-------+----------+--------+----------+---------+\n"
          ]
        }
      ],
      "source": [
        "!ngc registry model list nvidia/tao/lprnet:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dAx-bsIDPyoy"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $EXPERIMENT_DIR/pretrained_lprnet_baseline18/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8EieO4uYPyoy",
        "outputId": "0e859e9b-cedf-4c75-c536-781c249b2b4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting files to download...\n",
            "\u001b[?25l\u001b[32m⠋\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/22…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/22…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/22…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/22…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/22…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/22…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/22…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/22…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/22…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/22…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/22…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/22…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/22…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m \u001b[36m━━━━━━━\u001b[0m • \u001b[32m0.0/22…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m-:--:--\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 0 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m \u001b[36m━━━━━━\u001b[0m • \u001b[32m0.0/…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m21:03…\u001b[0m • \u001b[31m3.1  \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m \u001b[36m━━━━━━\u001b[0m • \u001b[32m0.0/…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:37:…\u001b[0m • \u001b[31m103.7\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m \u001b[36m━━━━━━\u001b[0m • \u001b[32m0.3/…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:04:…\u001b[0m • \u001b[31m914.7\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m \u001b[36m━━━━━━\u001b[0m • \u001b[32m1.7/…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m4.3  \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m \u001b[36m━━━━━━\u001b[0m • \u001b[32m6.3/…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m12.1 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m \u001b[36m━━━━━━\u001b[0m • \u001b[32m11.5…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m18.4 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m \u001b[36m━━━━━━\u001b[0m • \u001b[32m17.3…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m23.2 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m \u001b[33m╸\u001b[0m\u001b[36m━━━━━\u001b[0m • \u001b[32m23.0…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m26.9 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m \u001b[33m╸\u001b[0m\u001b[36m━━━━━\u001b[0m • \u001b[32m28.1…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m29.1 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m \u001b[33m╸\u001b[0m\u001b[36m━━━━━\u001b[0m • \u001b[32m33.8…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m31.4 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m \u001b[33m━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━━━━\u001b[0m • \u001b[32m39.1…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m32.8 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m \u001b[33m━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━━━━\u001b[0m • \u001b[32m44.4…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m34.3 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m \u001b[33m━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━━━━\u001b[0m • \u001b[32m49.7…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m35.2 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m \u001b[33m━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━━━━\u001b[0m • \u001b[32m55.0…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m36.1 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m \u001b[33m━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━━━━\u001b[0m • \u001b[32m60.4…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m48.8 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m \u001b[33m━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━━━━\u001b[0m • \u001b[32m66.2…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m48.8 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m \u001b[33m━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━━━━\u001b[0m • \u001b[32m71.9…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m49.4 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m \u001b[33m━━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━━━\u001b[0m • \u001b[32m77.4…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m49.3 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m \u001b[33m━━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━━━\u001b[0m • \u001b[32m83.3…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m49.6 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m \u001b[33m━━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━━━\u001b[0m • \u001b[32m88.5…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m49.6 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m \u001b[33m━━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━━━\u001b[0m • \u001b[32m94.1…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m49.6 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m \u001b[33m━━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━━━\u001b[0m • \u001b[32m99.8…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m49.8 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m \u001b[33m━━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━━━\u001b[0m • \u001b[32m105.…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m49.9 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m \u001b[33m━━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━━━\u001b[0m • \u001b[32m110.…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m49.5 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m \u001b[33m━━━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━━\u001b[0m • \u001b[32m114.…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m48.2 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m \u001b[33m━━━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━━\u001b[0m • \u001b[32m119.…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m47.9 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠇\u001b[0m \u001b[33m━━━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━━\u001b[0m • \u001b[32m124.…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m47.7 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m \u001b[33m━━━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━━\u001b[0m • \u001b[32m130.…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m48.0 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m \u001b[33m━━━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━━\u001b[0m • \u001b[32m136.…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m47.7 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m \u001b[33m━━━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━━\u001b[0m • \u001b[32m141.…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m47.5 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m \u001b[33m━━━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━━\u001b[0m • \u001b[32m147.…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m47.3 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m \u001b[33m━━━━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━\u001b[0m • \u001b[32m153.…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m47.5 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m \u001b[33m━━━━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━\u001b[0m • \u001b[32m157.…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m46.2 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m \u001b[33m━━━━\u001b[0m\u001b[36m╺\u001b[0m\u001b[36m━\u001b[0m • \u001b[32m161.…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m44.8 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m \u001b[33m━━━━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━\u001b[0m • \u001b[32m168.…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m45.3 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m \u001b[33m━━━━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━\u001b[0m • \u001b[32m172.…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m45.5 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m \u001b[33m━━━━\u001b[0m\u001b[33m╸\u001b[0m\u001b[36m━\u001b[0m • \u001b[32m178.…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m46.6 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m \u001b[33m━━━━━\u001b[0m\u001b[36m╺\u001b[0m • \u001b[32m184.…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m47.3 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m \u001b[33m━━━━━\u001b[0m\u001b[36m╺\u001b[0m • \u001b[32m189.…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m47.1 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠧\u001b[0m \u001b[33m━━━━━\u001b[0m\u001b[36m╺\u001b[0m • \u001b[32m194.…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m46.8 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠏\u001b[0m \u001b[33m━━━━━\u001b[0m\u001b[36m╺\u001b[0m • \u001b[32m200.…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m46.5 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m \u001b[33m━━━━━\u001b[0m\u001b[33m╸\u001b[0m • \u001b[32m205.…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m46.5 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m \u001b[33m━━━━━\u001b[0m\u001b[33m╸\u001b[0m • \u001b[32m211.…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m46.9 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m \u001b[33m━━━━━\u001b[0m\u001b[33m╸\u001b[0m • \u001b[32m217.…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m48.0 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 2 - Failed: 0\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2K  \u001b[90m━━━━━━\u001b[0m • \u001b[32m221.…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0:00:…\u001b[0m • \u001b[31m50.3 \u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0:00:…\u001b[0m • \u001b[34mTotal: 4 - Completed: 4 - Failed: 0\u001b[0m\n",
            "           \u001b[32mMiB  \u001b[0m                       \u001b[31mMB/s \u001b[0m                                                        \n",
            "\u001b[?25h\n",
            "----------------------------------------------------------------------------------------------------\n",
            "   Download status: COMPLETED\n",
            "   Downloaded local path model: /content/drive/MyDrive/results/lprnet/pretrained_lprnet_baseline18/lprnet_vtrainable_v1.0\n",
            "   Total files downloaded: 4\n",
            "   Total transferred: 221.06 MB\n",
            "   Started at: 2024-10-30 09:57:59\n",
            "   Completed at: 2024-10-30 09:58:06\n",
            "   Duration taken: 6s\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Pull pretrained model from NGC\n",
        "!ngc registry model download-version nvidia/tao/lprnet:trainable_v1.0 --dest $EXPERIMENT_DIR/pretrained_lprnet_baseline18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "m0ANlm3XPyoz",
        "outputId": "e41fa81b-1537-42f7-cee2-fdf9bdfe1729",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check that model is downloaded into dir.\n",
            "total 226365\n",
            "-rw------- 1 root root       200 Oct 30 09:58 ch_lp_characters.txt\n",
            "-rw------- 1 root root 115963904 Oct 30 09:58 ch_lprnet_baseline18_trainable.tlt\n",
            "-rw------- 1 root root        70 Oct 30 09:58 us_lp_characters.txt\n",
            "-rw------- 1 root root 115832832 Oct 30 09:58 us_lprnet_baseline18_trainable.tlt\n"
          ]
        }
      ],
      "source": [
        "print(\"Check that model is downloaded into dir.\")\n",
        "!ls -l $EXPERIMENT_DIR/pretrained_lprnet_baseline18/lprnet_vtrainable_v1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_26rCobXcri1"
      },
      "source": [
        "## 2. Setup GPU environment <a class=\"anchor\" id=\"head-2\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBV_YWiTc_KM"
      },
      "source": [
        "### 2.1 Setup Python environment <a class=\"anchor\" id=\"head-2-1\"></a>\n",
        "Setup the environment necessary to run the TAO Networks by running the bash script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "s2Xygw-y8fjm",
        "outputId": "9a4846de-fb78-46c3-cd93-c3fad5689601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "TAR file not found in the provided path",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-753eba32b50e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrt_tar_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TAR file not found in the provided path\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# FIXME 8: set to path of the folder where the TensoRT tar.gz file has to be untarred into\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: TAR file not found in the provided path"
          ]
        }
      ],
      "source": [
        "# FIXME 7: set this path of the uploaded TensorRT tar.gz file after browser download\n",
        "trt_tar_path=\"/content/drive/MyDrive/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0.tar.gz\"\n",
        "\n",
        "import os\n",
        "if not os.path.exists(trt_tar_path):\n",
        "  raise Exception(\"TAR file not found in the provided path\")\n",
        "\n",
        "# FIXME 8: set to path of the folder where the TensoRT tar.gz file has to be untarred into\n",
        "%env trt_untar_folder_path=/content/trt_untar\n",
        "# FIXME 9: set this to the version of TRT you have downloaded\n",
        "%env trt_version=8.6.1.6\n",
        "\n",
        "!sudo mkdir -p $trt_untar_folder_path && sudo chmod -R 777 $trt_untar_folder_path/\n",
        "\n",
        "import os\n",
        "\n",
        "untar = True\n",
        "for fname in os.listdir(os.environ.get(\"trt_untar_folder_path\", None)):\n",
        "  if fname.startswith(\"TensorRT-\"+os.environ.get(\"trt_version\")) and not fname.endswith(\".tar.gz\"):\n",
        "    untar = False\n",
        "\n",
        "if untar:\n",
        "  !tar -xzf $trt_tar_path -C /content/trt_untar\n",
        "\n",
        "if os.environ.get(\"LD_LIBRARY_PATH\",\"\") == \"\":\n",
        "  os.environ[\"LD_LIBRARY_PATH\"] = \"\"\n",
        "trt_lib_path = f':{os.environ.get(\"trt_untar_folder_path\")}/TensorRT-{os.environ.get(\"trt_version\")}/lib'\n",
        "os.environ[\"LD_LIBRARY_PATH\"]+=trt_lib_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zES4Dc2S1tzZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if os.environ[\"GOOGLE_COLAB\"] == \"1\":\n",
        "    os.environ[\"bash_script\"] = \"setup_env.sh\"\n",
        "else:\n",
        "    os.environ[\"bash_script\"] = \"setup_env_desktop.sh\"\n",
        "\n",
        "os.environ[\"NV_TAO_TF_TOP\"] = \"/tmp/tao_tensorflow1_backend/\"\n",
        "\n",
        "!sed -i \"s|PATH_TO_TRT|$trt_untar_folder_path|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n",
        "!sed -i \"s|TRT_VERSION|$trt_version|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n",
        "!sed -i \"s|PATH_TO_COLAB_NOTEBOOKS|$COLAB_NOTEBOOKS_PATH|g\" $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script\n",
        "\n",
        "!sh $COLAB_NOTEBOOKS_PATH/tensorflow/$bash_script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N3YWk16Pyoz"
      },
      "source": [
        "## 3. Provide training specification <a class=\"anchor\" id=\"head-3\"></a>\n",
        "\n",
        "* Note the spec $SPEC_DIR/default_sepc.txt is for training on US license plates:\n",
        "    * the max license plate length is 8;\n",
        "        * You can change `max_label_length` in `lpr_config` to satisfy your own dataset.\n",
        "    * the characters of US license plates are: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F, G, H, I, J, K, L, M, N, P, Q, R, S, T, U, V, W, X, Y, Z\n",
        "        * You can change `characters_list_file` in `dataset_config` to set your own characters.\n",
        "        * `characters_list_file` should contain all the characters in dataset. And one character takes one line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3fLUuoaPyo0",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!sed -i \"s|TAO_DATA_PATH|$DATA_DIR/|g\" $SPECS_DIR/tutorial_spec.txt\n",
        "!sed -i \"s|TAO_SPEC_DIR|$SPECS_DIR/|g\" $SPECS_DIR/tutorial_spec.txt\n",
        "!cat $SPECS_DIR/tutorial_spec.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yb9OAcd7Pyo0"
      },
      "outputs": [],
      "source": [
        "!cat $SPECS_DIR/us_lp_characters.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XifdFs5Pyo0"
      },
      "source": [
        "## 4. Run TAO training <a class=\"anchor\" id=\"head-4\"></a>\n",
        "* Provide the sample spec file and the output directory location for models\n",
        "* WARNING: training will take several hours or one day to complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnhmABuWPyo0"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $EXPERIMENT_DIR/experiment_dir_unpruned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8wKuOHNPyo1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(\"For multi-GPU, change --gpus based on your machine.\")\n",
        "!tao model lprnet train --gpus=1 --gpu_index=$GPU_INDEX \\\n",
        "                  -e $SPECS_DIR/tutorial_spec.txt \\\n",
        "                  -r $EXPERIMENT_DIR/experiment_dir_unpruned \\\n",
        "                  -k $KEY \\\n",
        "                  -m $EXPERIMENT_DIR/pretrained_lprnet_baseline18/lprnet_vtrainable_v1.0/us_lprnet_baseline18_trainable.tlt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7I87ZIdPyo1"
      },
      "outputs": [],
      "source": [
        "print(\"To resume training from a checkpoint, set the -m option to be the .tlt you want to resume from and --initial_epochs to be the epoch index of the resumed checkpoint\")\n",
        "# !tao model lprnet train --gpu_index=$GPU_INDEX \\\n",
        "#                   -e $SPECS_DIR/tutorial_spec.txt \\\n",
        "#                   -r $EXPERIMENT_DIR/experiment_dir_unpruned \\\n",
        "#                   -k $KEY \\\n",
        "#                   -m $EXPERIMENT_DIR/experiment_dir_unpruned/weights/lprnet_epoch-01.tlt\n",
        "#                   --initial_epoch 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDkWKQQkPyo1"
      },
      "outputs": [],
      "source": [
        "print('Model for each epoch:')\n",
        "print('---------------------')\n",
        "!ls -ltrh $EXPERIMENT_DIR/experiment_dir_unpruned/weights/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCYg-llnPyo1"
      },
      "source": [
        "## 5. Evaluate trained models <a class=\"anchor\" id=\"head-5\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELV8uJr-Pyo2",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!tao model lprnet evaluate --gpu_index=$GPU_INDEX -e $SPECS_DIR/tutorial_spec.txt \\\n",
        "                     -m $EXPERIMENT_DIR/experiment_dir_unpruned/weights/lprnet_epoch-024.hdf5 \\\n",
        "                     -k $KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvZE4H8KPyo2"
      },
      "source": [
        "## 6. Inferences <a class=\"anchor\" id=\"head-6\"></a>\n",
        "In this section, we run the lprnet inference tool to generate inferences on the trained models and print the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlqyTE7MPyo3"
      },
      "outputs": [],
      "source": [
        "# Running inference for detection on n images\n",
        "!tao model lprnet inference --gpu_index=$GPU_INDEX -i $DATA_DIR/val/image \\\n",
        "                      -e $SPECS_DIR/tutorial_spec.txt \\\n",
        "                      -m $EXPERIMENT_DIR/experiment_dir_unpruned/weights/lprnet_epoch-024.hdf5 \\\n",
        "                      -k $KEY"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "lprnet.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}